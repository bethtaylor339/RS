{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\betht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\betht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# from sklearn.surprise import Dataset, Reader\n",
    "# from surprise.model_selection import train_test_split\n",
    "# from surprise import SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('links.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "tags = pd.read_csv('tags.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>113497</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>113228</td>\n",
       "      <td>15602.0</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>114885</td>\n",
       "      <td>31357.0</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>113041</td>\n",
       "      <td>11862.0</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  imdbId   tmdbId                                        description\n",
       "0        1  114709    862.0  Led by Woody, Andy's toys live happily in his ...\n",
       "1        2  113497   8844.0  When siblings Judy and Peter discover an encha...\n",
       "2        3  113228  15602.0  A family wedding reignites the ancient feud be...\n",
       "3        4  114885  31357.0  Cheated on, mistreated and stepped on, the wom...\n",
       "4        5  113041  11862.0  Just when George Banks has recovered from his ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['description'] = ''\n",
    "\n",
    "# Define a function to fetch descriptions using TMDb API\n",
    "def fetch_description(tmdb_id, api_key):\n",
    "    url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "    params = {'api_key': api_key}\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data.get('overview', '')  # Return the movie description\n",
    "\n",
    "# Apply the function to each row in the DataFrame to fetch descriptions\n",
    "api_key = 'ab591caa973b321e21e39ee9544ce7ed'  # Replace with your actual API key\n",
    "for index, row in links.iterrows():\n",
    "    description = fetch_description(row['tmdbId'], api_key)\n",
    "    links.at[index, 'description'] = description\n",
    "\n",
    "# Save/load your dataset with descriptions\n",
    "links.to_csv('links_with_descriptions.csv', index=False)\n",
    "links = pd.read_csv('links_with_descriptions.csv')\n",
    "links.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in movies:\n",
      " movieId    0\n",
      "title      0\n",
      "genres     0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in ratings:\n",
      " userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in tags:\n",
      " userId       0\n",
      "movieId      0\n",
      "tag          0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in links:\n",
      " movieId          0\n",
      "imdbId           0\n",
      "tmdbId           8\n",
      "description    124\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betht\\AppData\\Local\\Temp\\ipykernel_1020\\3726481397.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  links['tmdbId'].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Checking and removing NaNs\n",
    "movies_nan = movies.isna().sum()\n",
    "ratings_nan = ratings.isna().sum()\n",
    "tags_nan = tags.isna().sum()\n",
    "links_nan = links.isna().sum()\n",
    "print(\"NaN counts in movies:\\n\", movies_nan)\n",
    "print(\"\\nNaN counts in ratings:\\n\", ratings_nan)\n",
    "print(\"\\nNaN counts in tags:\\n\", tags_nan)\n",
    "print(\"\\nNaN counts in links:\\n\", links_nan)\n",
    "\n",
    "# TMDB ID the only column with NaNs\n",
    "links['tmdbId'].fillna('Unknown', inplace=True)\n",
    "links['description'].fillna('No Description', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "ratings.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "tags.drop_duplicates(subset=['userId', 'movieId', 'tag'], inplace=True)\n",
    "\n",
    "# Data type conversion\n",
    "movies['movieId'] = movies['movieId'].astype(int)\n",
    "ratings['userId'] = ratings['userId'].astype(int)\n",
    "ratings['movieId'] = ratings['movieId'].astype(int)\n",
    "\n",
    "# Standardising text\n",
    "movies['title'] = movies['title'].str.lower()\n",
    "tags['tag'] = tags['tag'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]', '', x).lower())\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "\n",
    "# Extract year from the title and create a new 'year' column\n",
    "movies['year'] = movies['title'].str.extract(r'\\((\\d{4})\\)')\n",
    "\n",
    "# Remove the year from the 'title' column\n",
    "movies['title'] = movies['title'].str.rsplit(' (', n=1).str[0]\n",
    "\n",
    "# Changing format of timestamp\n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "tags['timestamp'] = pd.to_datetime(tags['timestamp'], unit='s')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp_x</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp_y</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-07-30 18:45:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-11-08 06:36:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2005-01-25 06:52:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2017-11-13 12:59:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2011-05-18 05:28:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId      title                                             genres  \\\n",
       "0        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "1        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "2        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "4        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "\n",
       "   year  userId  rating         timestamp_x  tag timestamp_y  imdbId tmdbId  \\\n",
       "0  1995       1     4.0 2000-07-30 18:45:03  NaN         NaT  114709  862.0   \n",
       "1  1995       5     4.0 1996-11-08 06:36:02  NaN         NaT  114709  862.0   \n",
       "2  1995       7     4.5 2005-01-25 06:52:26  NaN         NaT  114709  862.0   \n",
       "3  1995      15     2.5 2017-11-13 12:59:30  NaN         NaT  114709  862.0   \n",
       "4  1995      17     4.5 2011-05-18 05:28:03  NaN         NaT  114709  862.0   \n",
       "\n",
       "                                         description  \n",
       "0  Led by Woody, Andy's toys live happily in his ...  \n",
       "1  Led by Woody, Andy's toys live happily in his ...  \n",
       "2  Led by Woody, Andy's toys live happily in his ...  \n",
       "3  Led by Woody, Andy's toys live happily in his ...  \n",
       "4  Led by Woody, Andy's toys live happily in his ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ratings = pd.merge(movies, ratings, on='movieId')\n",
    "movie_ratings_tags = pd.merge(movie_ratings, tags, on=['movieId', 'userId'], how='left')\n",
    "final_dataset = pd.merge(movie_ratings_tags, links, on='movieId', how='left')\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in movies:\n",
      " movieId            0\n",
      "title              0\n",
      "genres             0\n",
      "year              20\n",
      "userId             0\n",
      "rating             0\n",
      "timestamp_x        0\n",
      "tag            99201\n",
      "timestamp_y    99201\n",
      "imdbId             0\n",
      "tmdbId             0\n",
      "description        0\n",
      "dtype: int64\n",
      "Number of non-NaN values in 'tag': 3476\n",
      "Number of non-NaN values in 'timestamp_y': 3476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tag</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-07-30 18:45:03</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-11-08 06:36:02</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2005-01-25 06:52:26</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2017-11-13 12:59:30</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2011-05-18 05:28:03</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId      title                                             genres  \\\n",
       "0        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "1        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "2        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "4        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "\n",
       "   year  userId  rating           timestamp     tag  imdbId tmdbId  \\\n",
       "0  1995       1     4.0 2000-07-30 18:45:03  No Tag  114709  862.0   \n",
       "1  1995       5     4.0 1996-11-08 06:36:02  No Tag  114709  862.0   \n",
       "2  1995       7     4.5 2005-01-25 06:52:26  No Tag  114709  862.0   \n",
       "3  1995      15     2.5 2017-11-13 12:59:30  No Tag  114709  862.0   \n",
       "4  1995      17     4.5 2011-05-18 05:28:03  No Tag  114709  862.0   \n",
       "\n",
       "                                         description  \n",
       "0  Led by Woody, Andy's toys live happily in his ...  \n",
       "1  Led by Woody, Andy's toys live happily in his ...  \n",
       "2  Led by Woody, Andy's toys live happily in his ...  \n",
       "3  Led by Woody, Andy's toys live happily in his ...  \n",
       "4  Led by Woody, Andy's toys live happily in his ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking NaNs\n",
    "final_dataset_nan = final_dataset.isna().sum()\n",
    "print(\"NaN counts in movies:\\n\", final_dataset_nan)\n",
    "# Counting non-NaN values in the 'tag' and 'timestamp_y' columns\n",
    "non_nan_count_tag = final_dataset['tag'].notna().sum()\n",
    "print(\"Number of non-NaN values in 'tag':\", non_nan_count_tag)\n",
    "non_nan_count_timestamp_y = final_dataset['timestamp_y'].notna().sum()\n",
    "print(\"Number of non-NaN values in 'timestamp_y':\", non_nan_count_timestamp_y)\n",
    "\n",
    "# Fill NaN values in 'tag' with 'No Tag'\n",
    "final_dataset['tag'].fillna('No Tag', inplace=True)\n",
    "\n",
    "# Drop 'timestamp_y'\n",
    "final_dataset.drop(columns=['timestamp_y'], inplace=True)\n",
    "final_dataset = final_dataset.rename(columns={'timestamp_x': 'timestamp'})\n",
    "final_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering- Timestamps to reflect trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# def get_movie_recommendations(user_summary):\n",
    "#     prompt = f\"Based on the following user preferences: {user_summary}, what are some movie recommendations?\"\n",
    "    \n",
    "#     response = openai.Completion.create(\n",
    "#       engine=\"text-davinci-004\",\n",
    "#       prompt=prompt,\n",
    "#       max_tokens=100\n",
    "#     )\n",
    "\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# # Example usage\n",
    "# user_summary = \"User A likes sci-fi movies, especially those directed by Christopher Nolan. They enjoyed 'Inception' and 'Interstellar' but didn't like '2001: A Space Odyssey'.\"\n",
    "# recommendations = get_movie_recommendations(user_summary)\n",
    "# print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS2- Neural Networks with trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_genre_list(genre_list):\n",
    "    # Join the list into a string\n",
    "    genre_string = ' '.join(genre_list)\n",
    "    # Clean the string\n",
    "    return clean_text(genre_string)\n",
    "\n",
    "# Apply the cleaning function to the 'genres' column\n",
    "final_dataset['genres'] = final_dataset['genres'].apply(clean_genre_list)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatize words (optional)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "final_dataset['description'] = final_dataset['description'].apply(clean_text)\n",
    "final_dataset['genres'] = final_dataset['genres'].apply(clean_text)\n",
    "final_dataset['tag'] = final_dataset['tag'].apply(clean_text)\n",
    "final_dataset['combined_features'] = final_dataset['genres'] + ' ' + final_dataset['tag'].fillna('') + ' ' + final_dataset['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max User ID: 609, Number of Users: 610\n",
      "Max Movie ID: 9723, Number of Movies: 9724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "user_id_encoder = LabelEncoder()\n",
    "ratings['userId'] = user_id_encoder.fit_transform(ratings['userId'])\n",
    "\n",
    "# Re-map movie IDs\n",
    "movie_id_encoder = LabelEncoder()\n",
    "ratings['movieId'] = movie_id_encoder.fit_transform(ratings['movieId'])\n",
    "\n",
    "# Now your num_users and num_movies will be\n",
    "num_users = ratings['userId'].nunique()\n",
    "num_movies = ratings['movieId'].nunique()\n",
    "# Check the max IDs to ensure they are within bounds\n",
    "max_user_id = ratings['userId'].max()\n",
    "max_movie_id = ratings['movieId'].max()\n",
    "print(f\"Max User ID: {max_user_id}, Number of Users: {num_users}\")\n",
    "print(f\"Max Movie ID: {max_movie_id}, Number of Movies: {num_movies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_11/embedding_23/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\betht\\AppData\\Local\\Temp\\ipykernel_1020\\715321410.py\", line 78, in <module>\n      model.fit(x_train, y_train, epochs=5, batch_size=32)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model_11/embedding_23/embedding_lookup'\nindices[3,0] = 69757 is not in [0, 9725)\n\t [[{{node model_11/embedding_23/embedding_lookup}}]] [Op:__inference_train_function_4242]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# Then follow with Steps 3, 4, and 5 as before.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m model \u001b[39m=\u001b[39m setup_neural_network(num_users, num_movies, embedding_size, nlp_feature_dim)\n\u001b[1;32m---> 78\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Step 4: Evaluate the model\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m# Replace `x_test` and `y_test` with your actual test data and targets.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_11/embedding_23/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\betht\\AppData\\Local\\Temp\\ipykernel_1020\\715321410.py\", line 78, in <module>\n      model.fit(x_train, y_train, epochs=5, batch_size=32)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model_11/embedding_23/embedding_lookup'\nindices[3,0] = 69757 is not in [0, 9725)\n\t [[{{node model_11/embedding_23/embedding_lookup}}]] [Op:__inference_train_function_4242]"
     ]
    }
   ],
   "source": [
    "# Step 1: Import TensorFlow and other required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "def setup_neural_network(num_users, num_movies, embedding_size, nlp_feature_dim):\n",
    "    # User and Movie Embeddings\n",
    "    user_input = Input(shape=(1,))\n",
    "    \n",
    "    user_embedding = Embedding(num_users+ 1, embedding_size, input_length=1)(user_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    \n",
    "    movie_input = Input(shape=(1,))\n",
    "    movie_embedding = Embedding(num_movies + 1, embedding_size, input_length=1)(movie_input)\n",
    "    movie_vec = Flatten()(movie_embedding)\n",
    "\n",
    "    # NLP Feature Input for movie descriptions\n",
    "    \n",
    "    nlp_input = Input(shape=(nlp_feature_dim,))\n",
    "    nlp_dense = Dense(embedding_size, activation='relu')(nlp_input)\n",
    "\n",
    "    # Combine Features\n",
    "    combined = Concatenate()([user_vec, movie_vec, nlp_dense])\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    dense = Dense(128, activation='relu')(combined)\n",
    "    prediction = Dense(1)(dense)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[user_input, movie_input, nlp_input], outputs=prediction)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "# You'll need to process your data to match the input shape expected by the model.\n",
    "# This typically involves encoding categorical data, normalizing inputs, and splitting into training and test sets.\n",
    "# Step 2: Preparing the Data\n",
    "# ... previous data processing steps ...\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Train the model\n",
    "# Replace `x_train` and `y_train` with your actual training data and targets.\n",
    "num_users = final_dataset['userId'].nunique()\n",
    "num_movies = final_dataset['movieId'].nunique()\n",
    "embedding_size = 100 #apparently best between 50 and 200\n",
    "nlp_feature_dim = 500\n",
    "\n",
    "\n",
    "# Process NLP features from movie descriptions using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=nlp_feature_dim)\n",
    "tfidf_matrix = tfidf.fit_transform(final_dataset['combined_features'].fillna(''))\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(final_dataset, test_size=0.2)\n",
    "# Prepare training data\n",
    "x_train = [\n",
    "    train_data['userId'].values,\n",
    "    train_data['movieId'].values,\n",
    "    tfidf_matrix[train_data.index].toarray()\n",
    "]\n",
    "y_train = train_data['rating'].values\n",
    "tfidf_matrix_test = tfidf.transform(final_dataset.loc[test_data.index, 'combined_features'].fillna(''))\n",
    "# Prepare testing data\n",
    "x_test = [\n",
    "    test_data['userId'].values,\n",
    "    test_data['movieId'].values,\n",
    "    tfidf_matrix[test_data.index].toarray()\n",
    "]\n",
    "y_test = test_data['rating'].values\n",
    "\n",
    "\n",
    "# Then follow with Steps 3, 4, and 5 as before.\n",
    "model = setup_neural_network(num_users, num_movies, embedding_size, nlp_feature_dim)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "# Replace `x_test` and `y_test` with your actual test data and targets.\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "# Step 5: Generate recommendations\n",
    "# Use the model to predict ratings for user-movie pairs that are not in the training data\n",
    "# and generate a list of recommended movies for each user.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content- based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_based_recommendations(title, cosine_sim, unrated_movies):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = unrated_movies.index[unrated_movies['title'] == title].tolist()[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Filter the movies to only include those that have not been rated by the user\n",
    "    similar_movies_ids = unrated_movies['movieId'].iloc[movie_indices]\n",
    "    return pd.DataFrame({'movieId': similar_movies_ids, 'title': unrated_movies['title'].iloc[movie_indices]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra data cleaning on movies and tags datasets \n",
    "movies['genres'] = movies['genres'].astype(str)\n",
    "tags['tag'] = tags['tag'].astype(str)\n",
    "\n",
    "# Combine genres and tags \n",
    "\n",
    "movies = movies.merge(tags, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "movies['metadata'] = movies['genres'] + ' ' + movies['tag']\n",
    "\n",
    "# Create a TF-IDF Vectorizer and transform the metadata to a matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['metadata'])\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get recommendations based on content similarity\n",
    "# def get_content_based_recommendations(title, cosine_sim=cosine_sim):\n",
    "#     # Get the index of the movie that matches the title\n",
    "#     idx = movies.index[movies['title'] == title].tolist()[0]\n",
    "\n",
    "#     # Get the pairwise similarity scores of all movies with that movie\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "#     # Sort the movies based on the similarity scores\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     # Get the scores of the 10 most similar movies\n",
    "#     sim_scores = sim_scores[1:11]\n",
    "\n",
    "#     # Get the movie indices\n",
    "#     movie_indices = [i[0] for i in sim_scores]\n",
    "#     similar_movies_ids = movies['movieId'].iloc[movie_indices]\n",
    "#     return pd.DataFrame({'movieId': similar_movies_ids, 'title': movies['title'].iloc[movie_indices]})\n",
    "def get_content_based_recommendations(title, cosine_sim, unrated_movies):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx_list = unrated_movies.index[unrated_movies['title'] == title].tolist()\n",
    "    \n",
    "    # Check if index list is empty\n",
    "    if not idx_list:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no match found\n",
    "\n",
    "    idx = idx_list[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices and validate them\n",
    "    movie_indices = [i[0] for i in sim_scores if i[0] < len(unrated_movies)]\n",
    "\n",
    "    # Filter the movies to only include those that have not been rated by the user\n",
    "    similar_movies_ids = unrated_movies['movieId'].iloc[movie_indices]\n",
    "    return pd.DataFrame({'movieId': similar_movies_ids, 'title': unrated_movies['title'].iloc[movie_indices]})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative merged with trending scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  hybrid_score\n",
      "0                                            tomorrow      3.680851\n",
      "1       when the levees broke: a requiem in four acts      3.679251\n",
      "2                days of being wild (a fei jingjyuhn)      3.659327\n",
      "3                                                 tag      3.649597\n",
      "4                                        shadow world      3.629711\n",
      "5                      kizumonogatari iii: cold blood      3.626845\n",
      "6   too funny to fail: the life and death of the d...      3.610003\n",
      "7               dc super hero girls: hero of the year      3.601991\n",
      "8   dragon ball z: the return of cooler (doragon b...      3.600709\n",
      "9                                          deadpool 2      3.597709\n",
      "10                                         deadpool 2      3.597709\n",
      "11                                         deadpool 2      3.597709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the ratings dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split the dataset\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "#Chosen algorithm\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Predict ratings for the testset\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Integrate trending scores in the recommendation phase\n",
    "def get_collaborative_recommendations(user_id, trending_scores, top_n=10):\n",
    "    # Predict ratings for all movies for the given user\n",
    "    movie_ids = ratings['movieId'].unique()\n",
    "    predicted_ratings = [svd.predict(user_id, movie_id).est for movie_id in movie_ids]\n",
    "\n",
    "    # Combine predictions with trending scores\n",
    "    movie_rating_trend = pd.DataFrame({'movieId': movie_ids, 'predicted_rating': predicted_ratings})\n",
    "    movie_rating_trend = movie_rating_trend.merge(trending_scores, on='movieId')\n",
    "\n",
    "    # Calculating a hybrid score\n",
    "    movie_rating_trend['hybrid_score'] = movie_rating_trend['predicted_rating'] * movie_rating_trend['normalised_score']\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_recommendations = movie_rating_trend.sort_values(by='hybrid_score', ascending=False).head(top_n)\n",
    "    return top_recommendations\n",
    "\n",
    "# Example usage\n",
    "user_id = 2  \n",
    "trending_scores = weighted_rating_score[['movieId', 'normalised_score']]  \n",
    "top_recommendations = get_collaborative_recommendations(user_id, trending_scores, top_n=10)\n",
    "\n",
    "\n",
    "top_recommendations = top_recommendations.merge(movies[['movieId', 'title']], on='movieId')\n",
    "print(top_recommendations[['title', 'hybrid_score']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining the 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       movieId  predicted_rating  normalised_score  hybrid_score  \\\n",
      "3       179135          3.863665          0.957879      3.700924   \n",
      "0       117364          3.823615          0.982147      3.755353   \n",
      "1       193609          3.759400          0.994013      3.736894   \n",
      "2        95473          3.740229          0.991033      3.706692   \n",
      "4       193585          3.724638          0.993019      3.698636   \n",
      "7216     32298               NaN               NaN           NaN   \n",
      "27          19               NaN               NaN           NaN   \n",
      "10388   112727               NaN               NaN           NaN   \n",
      "144        110               NaN               NaN           NaN   \n",
      "107         77               NaN               NaN           NaN   \n",
      "\n",
      "                                                   title  \n",
      "3                                         blue planet ii  \n",
      "0                                                virunga  \n",
      "1                           andrew dice clay: dice rules  \n",
      "2      dragon ball z: the return of cooler (doragon b...  \n",
      "4                                                  flint  \n",
      "7216                                           guess who  \n",
      "27                        ace ventura: when nature calls  \n",
      "10388                               deliver us from evil  \n",
      "144                                           braveheart  \n",
      "107                                            nico icon  \n"
     ]
    }
   ],
   "source": [
    "def get_unified_recommendations(user_id, ratings, trending_scores, tfidf_matrix, cosine_sim, movies, top_n=10):\n",
    "    # Filter out movies already rated by the user\n",
    "    rated_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].unique()\n",
    "    unrated_movies = movies[~movies['movieId'].isin(rated_movie_ids)]\n",
    "\n",
    "    # Get collaborative recommendations\n",
    "    collaborative_recs = get_collaborative_recommendations(user_id, trending_scores, 5)\n",
    "    collaborative_recs = collaborative_recs.merge(unrated_movies[['movieId', 'title']], on='movieId', how='left')\n",
    "\n",
    "    # Initialize an empty DataFrame for content-based recommendations\n",
    "    content_based_recs = pd.DataFrame()\n",
    "\n",
    "    for movie_id in collaborative_recs['movieId']:\n",
    "        if movie_id in rated_movie_ids:  # Skip if the movie is already rated\n",
    "            continue\n",
    "\n",
    "        title = unrated_movies[unrated_movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "        similar_movies = get_content_based_recommendations(title, cosine_sim, unrated_movies)\n",
    "\n",
    "        if not similar_movies.empty:\n",
    "            top_similar = similar_movies.head(1)\n",
    "            if top_similar['movieId'].iloc[0] not in rated_movie_ids:  # Avoid recommending already rated movies\n",
    "                content_based_recs = pd.concat([content_based_recs, top_similar])\n",
    "\n",
    "    # Combine collaborative and content-based recommendations\n",
    "    unified_recommendations = pd.concat([collaborative_recs, content_based_recs]).drop_duplicates(subset=['movieId'])\n",
    "\n",
    "    # Sort by predicted rating (if available in your data)\n",
    "    if 'predicted_rating' in unified_recommendations.columns:\n",
    "        unified_recommendations = unified_recommendations.sort_values(by='predicted_rating', ascending=False)\n",
    "\n",
    "    # Limit to top N recommendations\n",
    "    unified_recommendations = unified_recommendations.head(top_n)\n",
    "\n",
    "    return unified_recommendations\n",
    "\n",
    "# Example usage\n",
    "user_id = 4  # Replace with an actual user ID from your dataset\n",
    "unified_recommendations = get_unified_recommendations(user_id, ratings, trending_scores, tfidf_matrix, cosine_sim, movies, top_n=10)\n",
    "\n",
    "# Display the recommendations\n",
    "print(unified_recommendations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.012872185911401601\n",
      "Average Recall: 0.00445609150211172\n",
      "Average MRR: 0.02051302002282394\n",
      "Average MAP: 0.002469412758747754\n",
      "Average NDCG: 0.011087682307154166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to hide some of a user's ratings\n",
    "def hide_user_ratings(user_id, ratings, hide_ratio=0.25):\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    hide_indices = np.random.choice(user_ratings.index, size=int(len(user_ratings) * hide_ratio), replace=False)\n",
    "    hidden_ratings = user_ratings.loc[hide_indices]\n",
    "    visible_ratings = user_ratings.drop(hide_indices)\n",
    "    return visible_ratings, hidden_ratings\n",
    "\n",
    "# Function to calculate MAP\n",
    "def calculate_map(recommendations, hidden_ids):\n",
    "    relevant = 0\n",
    "    total_precision = 0\n",
    "    for k, rec in enumerate(recommendations['movieId'], start=1):\n",
    "        if rec in hidden_ids:\n",
    "            relevant += 1\n",
    "            total_precision += relevant / k\n",
    "    return total_precision / len(hidden_ids) if hidden_ids else 0\n",
    "\n",
    "# Function to calculate NDCG\n",
    "def calculate_ndcg(recommendations, hidden_ids, k=10):\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, rec in enumerate(recommendations['movieId'].head(k)) if rec in hidden_ids)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(hidden_ids), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Function to calculate MRR\n",
    "def calculate_mrr(recommendations, hidden_ids):\n",
    "    for i, rec in enumerate(recommendations['movieId'], start=1):\n",
    "        if rec in hidden_ids:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "# Function to calculate precision and recall\n",
    "def calculate_precision_recall(recommendations, hidden_ratings, k=10):\n",
    "    recommended_ids = set(recommendations['movieId'].head(k))\n",
    "    hidden_ids = set(hidden_ratings['movieId'])\n",
    "    hits = recommended_ids.intersection(hidden_ids)\n",
    "    precision = len(hits) / len(recommended_ids)\n",
    "    recall = len(hits) / len(hidden_ids) if hidden_ids else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Split users into train and test sets\n",
    "unique_users = ratings['userId'].unique()\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each user\n",
    "precisions = []\n",
    "recalls = []\n",
    "mrrs = []\n",
    "maps = []\n",
    "ndcgs = []\n",
    "\n",
    "# Evaluate recommendations for each user in the test set\n",
    "for user_id in test_users:\n",
    "    visible_ratings, hidden_ratings = hide_user_ratings(user_id, ratings)\n",
    "    visible_ratings = pd.concat([ratings[ratings['userId'] != user_id], visible_ratings])\n",
    "\n",
    "    recommendations = get_unified_recommendations(user_id, visible_ratings, trending_scores, tfidf_matrix, cosine_sim, movies, top_n=10)\n",
    "    \n",
    "    # Extract hidden_ids for the current user\n",
    "    hidden_ids = set(hidden_ratings['movieId'])\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall = calculate_precision_recall(recommendations, hidden_ratings, k=10)\n",
    "    mrr = calculate_mrr(recommendations, hidden_ids)\n",
    "    map_score = calculate_map(recommendations, hidden_ids)\n",
    "    ndcg = calculate_ndcg(recommendations, hidden_ids, k=10)\n",
    "\n",
    "    # Append to lists\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    mrrs.append(mrr)\n",
    "    maps.append(map_score)\n",
    "    ndcgs.append(ndcg)\n",
    "\n",
    "# Calculate average of all metrics across all users\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_mrr = sum(mrrs) / len(mrrs)\n",
    "avg_map = sum(maps) / len(maps)\n",
    "avg_ndcg = sum(ndcgs) / len(ndcgs)\n",
    "\n",
    "print(f'Average Precision: {avg_precision}')\n",
    "print(f'Average Recall: {avg_recall}')\n",
    "print(f'Average MRR: {avg_mrr}')\n",
    "print(f'Average MAP: {avg_map}')\n",
    "print(f'Average NDCG: {avg_ndcg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc62410c1dc1ccf5d5996d34860afc84be3b626c399c1f0d0d93f37d8f732fd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
