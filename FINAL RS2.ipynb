{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\betht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\betht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('links.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "tags = pd.read_csv('tags.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>113497</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>113228</td>\n",
       "      <td>15602.0</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>114885</td>\n",
       "      <td>31357.0</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>113041</td>\n",
       "      <td>11862.0</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  imdbId   tmdbId                                        description\n",
       "0        1  114709    862.0  Led by Woody, Andy's toys live happily in his ...\n",
       "1        2  113497   8844.0  When siblings Judy and Peter discover an encha...\n",
       "2        3  113228  15602.0  A family wedding reignites the ancient feud be...\n",
       "3        4  114885  31357.0  Cheated on, mistreated and stepped on, the wom...\n",
       "4        5  113041  11862.0  Just when George Banks has recovered from his ..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['description'] = ''\n",
    "# Fetching descriptions from TMDB API\n",
    "# Using https://www.codespeedy.com/fetch-tmdb-movie-data-using-python/\n",
    "def fetch_description(tmdb_id, api_key):\n",
    "    url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "    params = {'api_key': api_key}\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data.get('overview', '') \n",
    "# Applyiing function to each row\n",
    "api_key = 'ab591caa973b321e21e39ee9544ce7ed'  \n",
    "for index, row in links.iterrows():\n",
    "    description = fetch_description(row['tmdbId'], api_key)\n",
    "    links.at[index, 'description'] = description\n",
    "\n",
    "# Save & loading dataset with descriptions\n",
    "links.to_csv('links_with_descriptions.csv', index=False)\n",
    "links = pd.read_csv('links_with_descriptions.csv')\n",
    "links.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in movies:\n",
      " movieId    0\n",
      "title      0\n",
      "genres     0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in ratings:\n",
      " userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in tags:\n",
      " userId       0\n",
      "movieId      0\n",
      "tag          0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "NaN counts in links:\n",
      " movieId          0\n",
      "imdbId           0\n",
      "tmdbId           8\n",
      "description    124\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betht\\AppData\\Local\\Temp\\ipykernel_1020\\3726481397.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  links['tmdbId'].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Checking and removing NaNs\n",
    "movies_nan = movies.isna().sum()\n",
    "ratings_nan = ratings.isna().sum()\n",
    "tags_nan = tags.isna().sum()\n",
    "links_nan = links.isna().sum()\n",
    "print(\"NaN counts in movies:\\n\", movies_nan)\n",
    "print(\"\\nNaN counts in ratings:\\n\", ratings_nan)\n",
    "print(\"\\nNaN counts in tags:\\n\", tags_nan)\n",
    "print(\"\\nNaN counts in links:\\n\", links_nan)\n",
    "\n",
    "# TMDB ID the only column with NaNs\n",
    "links['tmdbId'].fillna('Unknown', inplace=True)\n",
    "links['description'].fillna('No Description', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "ratings.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "tags.drop_duplicates(subset=['userId', 'movieId', 'tag'], inplace=True)\n",
    "\n",
    "# Data type conversion\n",
    "movies['movieId'] = movies['movieId'].astype(int)\n",
    "ratings['userId'] = ratings['userId'].astype(int)\n",
    "ratings['movieId'] = ratings['movieId'].astype(int)\n",
    "\n",
    "# Standardising text\n",
    "movies['title'] = movies['title'].str.lower()\n",
    "tags['tag'] = tags['tag'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]', '', x).lower())\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "\n",
    "# Extract year from the title and create a new 'year' column\n",
    "movies['year'] = movies['title'].str.extract(r'\\((\\d{4})\\)')\n",
    "\n",
    "# Remove the year from the 'title' column\n",
    "movies['title'] = movies['title'].str.rsplit(' (', n=1).str[0]\n",
    "\n",
    "# Changing format of timestamp\n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "tags['timestamp'] = pd.to_datetime(tags['timestamp'], unit='s')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = pd.merge(movies, ratings, on='movieId')\n",
    "movie_ratings_tags = pd.merge(movie_ratings, tags, on=['movieId', 'userId'], how='left')\n",
    "final_dataset = pd.merge(movie_ratings_tags, links, on='movieId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in movies:\n",
      " movieId            0\n",
      "title              0\n",
      "genres             0\n",
      "year              20\n",
      "userId             0\n",
      "rating             0\n",
      "timestamp_x        0\n",
      "tag            99201\n",
      "timestamp_y    99201\n",
      "imdbId             0\n",
      "tmdbId             0\n",
      "description        0\n",
      "dtype: int64\n",
      "Number of non-NaN values in 'tag': 3476\n",
      "Number of non-NaN values in 'timestamp_y': 3476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>year</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tag</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000-07-30 18:45:03</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-11-08 06:36:02</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2005-01-25 06:52:26</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2017-11-13 12:59:30</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>toy story</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>1995</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2011-05-18 05:28:03</td>\n",
       "      <td>No Tag</td>\n",
       "      <td>114709</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId      title                                             genres  \\\n",
       "0        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "1        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "2        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "3        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "4        1  toy story  [Adventure, Animation, Children, Comedy, Fantasy]   \n",
       "\n",
       "   year  userId  rating           timestamp     tag  imdbId tmdbId  \\\n",
       "0  1995       1     4.0 2000-07-30 18:45:03  No Tag  114709  862.0   \n",
       "1  1995       5     4.0 1996-11-08 06:36:02  No Tag  114709  862.0   \n",
       "2  1995       7     4.5 2005-01-25 06:52:26  No Tag  114709  862.0   \n",
       "3  1995      15     2.5 2017-11-13 12:59:30  No Tag  114709  862.0   \n",
       "4  1995      17     4.5 2011-05-18 05:28:03  No Tag  114709  862.0   \n",
       "\n",
       "                                         description  \n",
       "0  Led by Woody, Andy's toys live happily in his ...  \n",
       "1  Led by Woody, Andy's toys live happily in his ...  \n",
       "2  Led by Woody, Andy's toys live happily in his ...  \n",
       "3  Led by Woody, Andy's toys live happily in his ...  \n",
       "4  Led by Woody, Andy's toys live happily in his ...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking NaNs\n",
    "final_dataset_nan = final_dataset.isna().sum()\n",
    "print(\"NaN counts in movies:\\n\", final_dataset_nan)\n",
    "\n",
    "# Counting non-NaN values in the 'tag' and 'timestamp_y' columns\n",
    "non_nan_count_tag = final_dataset['tag'].notna().sum()\n",
    "print(\"Number of non-NaN values in 'tag':\", non_nan_count_tag)\n",
    "\n",
    "non_nan_count_timestamp_y = final_dataset['timestamp_y'].notna().sum()\n",
    "print(\"Number of non-NaN values in 'timestamp_y':\", non_nan_count_timestamp_y)\n",
    "\n",
    "# Fill NaN values in 'tag' with 'No Tag'\n",
    "final_dataset['tag'].fillna('No Tag', inplace=True)\n",
    "\n",
    "# Drop 'timestamp_y'\n",
    "final_dataset.drop(columns=['timestamp_y'], inplace=True)\n",
    "final_dataset = final_dataset.rename(columns={'timestamp_x': 'timestamp'})\n",
    "final_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS2- Neural Networks with trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing for NLP\n",
    "def clean_genre_list(genre_list):\n",
    "    # Join the list into a string\n",
    "    genre_string = ' '.join(genre_list)\n",
    "    # Clean the string\n",
    "    return clean_text(genre_string)\n",
    "\n",
    "# Apply the cleaning function to the 'genres' column\n",
    "final_dataset['genres'] = final_dataset['genres'].apply(clean_genre_list)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatize words (optional)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "final_dataset['description'] = final_dataset['description'].apply(clean_text)\n",
    "final_dataset['genres'] = final_dataset['genres'].apply(clean_text)\n",
    "final_dataset['tag'] = final_dataset['tag'].apply(clean_text)\n",
    "final_dataset['combined_features'] = final_dataset['genres'] + ' ' + final_dataset['tag'].fillna('') + ' ' + final_dataset['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max User ID: 609, Number of Users: 610\n",
      "Max Movie ID: 9723, Number of Movies: 9724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "user_id_encoder = LabelEncoder()\n",
    "final_dataset['userId'] = user_id_encoder.fit_transform(final_dataset['userId'])\n",
    "\n",
    "# Re-map movie IDs\n",
    "movie_id_encoder = LabelEncoder()\n",
    "final_dataset['movieId'] = movie_id_encoder.fit_transform(final_dataset['movieId'])\n",
    "\n",
    "# Now your num_users and num_movies will be\n",
    "num_users = final_dataset['userId'].nunique()\n",
    "num_movies = final_dataset['movieId'].nunique()\n",
    "# Check the max IDs to ensure they are within bounds\n",
    "max_user_id = final_dataset['userId'].max()\n",
    "max_movie_id = final_dataset['movieId'].max()\n",
    "print(f\"Max User ID: {max_user_id}, Number of Users: {num_users}\")\n",
    "print(f\"Max Movie ID: {max_movie_id}, Number of Movies: {num_movies}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (102677, 500)\n",
      "NLP feature dimension: 500\n",
      "Shapes of x_train arrays: [(82141,), (82141,), (82141, 500)]\n",
      "Shapes of x_test arrays: [(20536,), (20536,), (20536, 500)]\n",
      "Epoch 1/5\n",
      "2567/2567 [==============================] - 18s 6ms/step - loss: 0.9502\n",
      "Epoch 2/5\n",
      "2567/2567 [==============================] - 16s 6ms/step - loss: 0.7345\n",
      "Epoch 3/5\n",
      "2567/2567 [==============================] - 16s 6ms/step - loss: 0.6805\n",
      "Epoch 4/5\n",
      "2567/2567 [==============================] - 17s 7ms/step - loss: 0.6434\n",
      "Epoch 5/5\n",
      "2567/2567 [==============================] - 18s 7ms/step - loss: 0.6171\n",
      "642/642 [==============================] - 2s 3ms/step - loss: 0.7468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7468015551567078"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import TensorFlow and other required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "def setup_neural_network(num_users, num_movies, embedding_size, nlp_feature_dim):\n",
    "    # User and Movie Embeddings\n",
    "    user_input = Input(shape=(1,))\n",
    "    \n",
    "    user_embedding = Embedding(num_users+ 1, embedding_size, input_length=1)(user_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    \n",
    "    movie_input = Input(shape=(1,))\n",
    "    movie_embedding = Embedding(num_movies + 1, embedding_size, input_length=1)(movie_input)\n",
    "    movie_vec = Flatten()(movie_embedding)\n",
    "\n",
    "    # NLP Feature Input for movie descriptions\n",
    "    \n",
    "    nlp_input = Input(shape=(nlp_feature_dim,))\n",
    "    nlp_dense = Dense(embedding_size, activation='relu')(nlp_input)\n",
    "\n",
    "    # Combine Features\n",
    "    combined = Concatenate()([user_vec, movie_vec, nlp_dense])\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    dense = Dense(128, activation='relu')(combined)\n",
    "    prediction = Dense(1)(dense)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=[user_input, movie_input, nlp_input], outputs=prediction)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "num_users = final_dataset['userId'].nunique()\n",
    "num_movies = final_dataset['movieId'].nunique()\n",
    "embedding_size = 100 \n",
    "nlp_feature_dim = 500\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=nlp_feature_dim)\n",
    "tfidf_matrix = tfidf.fit_transform(final_dataset['combined_features'].fillna(''))\n",
    "\n",
    "train_data, test_data = train_test_split(final_dataset, test_size=0.2)\n",
    "\n",
    "x_train = [\n",
    "    train_data['userId'].values,\n",
    "    train_data['movieId'].values,\n",
    "    tfidf_matrix[train_data.index].toarray()\n",
    "]\n",
    "y_train = train_data['rating'].values\n",
    "tfidf_matrix_test = tfidf.transform(final_dataset.loc[test_data.index, 'combined_features'].fillna(''))\n",
    "# Prepare testing data\n",
    "x_test = [\n",
    "    test_data['userId'].values,\n",
    "    test_data['movieId'].values,\n",
    "    tfidf_matrix[test_data.index].toarray()\n",
    "]\n",
    "y_test = test_data['rating'].values\n",
    "# Check the shape of the TF-IDF matrix\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Ensure the feature dimension matches\n",
    "print(\"NLP feature dimension:\", nlp_feature_dim)\n",
    "\n",
    "# Check the shapes of x_train and x_test arrays\n",
    "print(\"Shapes of x_train arrays:\", [arr.shape for arr in x_train])\n",
    "print(\"Shapes of x_test arrays:\", [arr.shape for arr in x_test])\n",
    "\n",
    "\n",
    "model = setup_neural_network(num_users, num_movies, embedding_size, nlp_feature_dim)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 54s 201ms/step - loss: 0.0013\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 51s 207ms/step - loss: 0.0013\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 297s 1s/step - loss: 0.0013\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 46s 188ms/step - loss: 0.0013\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 43s 175ms/step - loss: 0.0013\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 48s 198ms/step - loss: 0.0013\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 57s 233ms/step - loss: 0.0013\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 49s 199ms/step - loss: 0.0013\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 53s 217ms/step - loss: 0.0013\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 49s 201ms/step - loss: 0.0013\n",
      "304/304 [==============================] - 19s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "# Example: Aggregate ratings by month for each movie\n",
    "ratings['month_year'] = ratings['timestamp'].dt.to_period('M')\n",
    "monthly_ratings = ratings.groupby(['movieId', 'month_year']).size().unstack(fill_value=0)\n",
    "\n",
    "# Normalize the data (optional)\n",
    "# You can use Min-Max scaling or another appropriate method here\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Assuming monthly_ratings is your prepared time-series data\n",
    "# Reshape data for LSTM (samples, time steps, features)\n",
    "X = monthly_ratings.values.reshape((monthly_ratings.shape[0], monthly_ratings.shape[1], 1))\n",
    "monthly_diff = monthly_ratings.shift(-1, axis=1) - monthly_ratings\n",
    "\n",
    "# Define a threshold for what you consider as 'trending'\n",
    "trend_threshold = 10  # Example threshold\n",
    "\n",
    "# Create a binary trend indicator (1 for trending, 0 for not trending)\n",
    "# Here, we consider a movie 'trending' if its rating count increases by trend_threshold from one month to the next\n",
    "is_trending = (monthly_diff >= trend_threshold).any(axis=1).astype(int)\n",
    "\n",
    "# Assign the binary trend indicator to y\n",
    "y = is_trending.values\n",
    " # Define your target variable (e.g., future ratings, trend indicator)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Build and train the LSTM model\n",
    "lstm_model = build_lstm_model((X_train.shape[1], 1))\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# Predict trends\n",
    "predicted_trends = lstm_model.predict(X)\n",
    "\n",
    "# Integrate these trends with your recommendation system\n",
    "# For example, adjust the predicted ratings based on the trend\n",
    "# ...\n",
    "\n",
    "# Proceed with generating ranked recommendations as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3203/3203 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:4484\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int32HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:4508\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int32HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m user_id \u001b[39m=\u001b[39m \u001b[39m123\u001b[39m  \u001b[39m# Replace with the user ID for whom you want recommendations\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m unified_recommendations \u001b[39m=\u001b[39m get_unified_recommendations(user_id, model, lstm_model, tfidf, final_dataset, top_n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Display the recommendations\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39mprint\u001b[39m(unified_recommendations)\n",
      "Cell \u001b[1;32mIn[83], line 26\u001b[0m, in \u001b[0;36mget_unified_recommendations\u001b[1;34m(user_id, model, lstm_model, tfidf, final_dataset, top_n)\u001b[0m\n\u001b[0;32m     24\u001b[0m lstm_predictions \u001b[39m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m movie_id \u001b[39min\u001b[39;00m movies_to_predict[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m---> 26\u001b[0m     lstm_input \u001b[39m=\u001b[39m get_lstm_input(movie_id, monthly_ratings)\n\u001b[0;32m     27\u001b[0m     trend_score \u001b[39m=\u001b[39m lstm_model\u001b[39m.\u001b[39mpredict(lstm_input)\n\u001b[0;32m     28\u001b[0m     lstm_predictions\u001b[39m.\u001b[39mappend(trend_score\u001b[39m.\u001b[39mflatten()[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[83], line 2\u001b[0m, in \u001b[0;36mget_lstm_input\u001b[1;34m(movie_id, monthly_ratings)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lstm_input\u001b[39m(movie_id, monthly_ratings):\n\u001b[1;32m----> 2\u001b[0m     lstm_input \u001b[39m=\u001b[39m monthly_ratings\u001b[39m.\u001b[39;49mloc[movie_id]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m lstm_input\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:1393\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1393\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1342\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:4236\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4234\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[0;32m   4235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4236\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   4238\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   4239\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[1;32mc:\\Users\\betht\\anaconda3\\envs\\myenv\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def get_lstm_input(movie_id, monthly_ratings):\n",
    "    lstm_input = monthly_ratings.loc[movie_id].values.reshape((1, -1, 1))\n",
    "    return lstm_input\n",
    "def get_unified_recommendations(user_id, model, lstm_model, tfidf, final_dataset, top_n=10):\n",
    "    # Fetch movies not yet rated by the user\n",
    "    user_movies = ratings[ratings['userId'] == user_id]['movieId']\n",
    "    movies_to_predict = final_dataset[~final_dataset['movieId'].isin(user_movies)]\n",
    "\n",
    "    # Prepare data for the NLP-based model\n",
    "    tfidf_features = tfidf.transform(movies_to_predict['combined_features'].fillna('')).toarray()\n",
    "    x_user_movies = [\n",
    "        np.full(tfidf_features.shape[0], user_id),  # User ID array\n",
    "        movies_to_predict['movieId'].values,        # Movie IDs\n",
    "        tfidf_features                              # TF-IDF features\n",
    "    ]\n",
    "\n",
    "    # Predict ratings with the NLP-based model\n",
    "    predicted_ratings = model.predict(x_user_movies).flatten()\n",
    "\n",
    "    # Get trend scores for these movies\n",
    "    # Assuming lstm_model is your trained LSTM model for trend prediction\n",
    "    # You will need to prepare the input for the LSTM model as per its requirements\n",
    "    # Predict trend scores\n",
    "    lstm_predictions = []\n",
    "    for movie_id in movies_to_predict['movieId']:\n",
    "        lstm_input = get_lstm_input(movie_id, monthly_ratings)\n",
    "        trend_score = lstm_model.predict(lstm_input)\n",
    "        lstm_predictions.append(trend_score.flatten()[0])\n",
    "\n",
    "    lstm_predictions = np.array(lstm_predictions)\n",
    " \n",
    "\n",
    "    # Combine the predictions (you can adjust how you combine these)\n",
    "    combined_scores = predicted_ratings + lstm_predictions\n",
    "\n",
    "    # Create a DataFrame for sorting and filtering\n",
    "    recommendations = pd.DataFrame({\n",
    "        'movieId': movies_to_predict['movieId'],\n",
    "        'predicted_rating': combined_scores\n",
    "    })\n",
    "\n",
    "    # Sort by combined score and fetch top N movies\n",
    "    top_recommendations = recommendations.sort_values(by='predicted_rating', ascending=False).head(top_n)\n",
    "\n",
    "    # Fetch movie details from the movies DataFrame\n",
    "    movie_details = movies[movies['movieId'].isin(top_recommendations['movieId'])]\n",
    "\n",
    "    return movie_details\n",
    "\n",
    "# Example usage\n",
    "user_id = 123  # Replace with the user ID for whom you want recommendations\n",
    "unified_recommendations = get_unified_recommendations(user_id, model, lstm_model, tfidf, final_dataset, top_n=10)\n",
    "\n",
    "# Display the recommendations\n",
    "print(unified_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data & Evaluation (Copied over from RS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to hide some of a user's ratings\n",
    "def hide_user_ratings(user_id, ratings, hide_ratio=0.25):\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    hide_indices = np.random.choice(user_ratings.index, size=int(len(user_ratings) * hide_ratio), replace=False)\n",
    "    hidden_ratings = user_ratings.loc[hide_indices]\n",
    "    visible_ratings = user_ratings.drop(hide_indices)\n",
    "    return visible_ratings, hidden_ratings\n",
    "\n",
    "# Function to calculate MAP\n",
    "def calculate_map(recommendations, hidden_ids):\n",
    "    relevant = 0\n",
    "    total_precision = 0\n",
    "    for k, rec in enumerate(recommendations['movieId'], start=1):\n",
    "        if rec in hidden_ids:\n",
    "            relevant += 1\n",
    "            total_precision += relevant / k\n",
    "    return total_precision / len(hidden_ids) if hidden_ids else 0\n",
    "\n",
    "# Function to calculate NDCG\n",
    "def calculate_ndcg(recommendations, hidden_ids, k=10):\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, rec in enumerate(recommendations['movieId'].head(k)) if rec in hidden_ids)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(hidden_ids), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Function to calculate MRR\n",
    "def calculate_mrr(recommendations, hidden_ids):\n",
    "    for i, rec in enumerate(recommendations['movieId'], start=1):\n",
    "        if rec in hidden_ids:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "# Function to calculate precision and recall\n",
    "def calculate_precision_recall(recommendations, hidden_ratings, k=10):\n",
    "    recommended_ids = set(recommendations['movieId'].head(k))\n",
    "    hidden_ids = set(hidden_ratings['movieId'])\n",
    "    hits = recommended_ids.intersection(hidden_ids)\n",
    "    precision = len(hits) / len(recommended_ids)\n",
    "    recall = len(hits) / len(hidden_ids) if hidden_ids else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Split users into train and test sets\n",
    "unique_users = ratings['userId'].unique()\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each user\n",
    "precisions = []\n",
    "recalls = []\n",
    "mrrs = []\n",
    "maps = []\n",
    "ndcgs = []\n",
    "\n",
    "# Evaluate recommendations for each user in the test set\n",
    "for user_id in test_users:\n",
    "    visible_ratings, hidden_ratings = hide_user_ratings(user_id, ratings)\n",
    "    visible_ratings = pd.concat([ratings[ratings['userId'] != user_id], visible_ratings])\n",
    "\n",
    "    recommendations = get_unified_recommendations(user_id, visible_ratings, trending_scores, tfidf_matrix, cosine_sim, movies, top_n=10)\n",
    "    \n",
    "    # Extract hidden_ids for the current user\n",
    "    hidden_ids = set(hidden_ratings['movieId'])\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall = calculate_precision_recall(recommendations, hidden_ratings, k=10)\n",
    "    mrr = calculate_mrr(recommendations, hidden_ids)\n",
    "    map_score = calculate_map(recommendations, hidden_ids)\n",
    "    ndcg = calculate_ndcg(recommendations, hidden_ids, k=10)\n",
    "\n",
    "    # Append to lists\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    mrrs.append(mrr)\n",
    "    maps.append(map_score)\n",
    "    ndcgs.append(ndcg)\n",
    "\n",
    "# Calculate average of all metrics across all users\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_mrr = sum(mrrs) / len(mrrs)\n",
    "avg_map = sum(maps) / len(maps)\n",
    "avg_ndcg = sum(ndcgs) / len(ndcgs)\n",
    "\n",
    "print(f'Average Precision: {avg_precision}')\n",
    "print(f'Average Recall: {avg_recall}')\n",
    "print(f'Average MRR: {avg_mrr}')\n",
    "print(f'Average MAP: {avg_map}')\n",
    "print(f'Average NDCG: {avg_ndcg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc62410c1dc1ccf5d5996d34860afc84be3b626c399c1f0d0d93f37d8f732fd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
